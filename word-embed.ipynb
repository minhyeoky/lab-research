{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "# os.environ['TF_KERAS'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "# import keras_bert\n",
    "from functools import partial\n",
    "import importlib\n",
    "import data_loader\n",
    "importlib.reload(data_loader)\n",
    "print(tf.__version__)\n",
    "DataLoader = data_loader.DataLoader\n",
    "\n",
    "# keras_bert.backend.TF_KERAS = 1\n",
    "\n",
    "config = './config/hub2-1.json'\n",
    "# print(keras_bert.backend.TF_KERAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:DataLoader initializing\n",
      "INFO:root:reading lab data from ../data/soundAttGAN & ../data/soundAttGAN/koreancorpus.xlsx\n",
      "INFO:root:total number of data: 1925\n",
      "INFO:root:reading hub data from ../data/KsponSpeech_01\n",
      "DEBUG:root:../data/KsponSpeech_01/KsponSpeech_0094\n",
      "INFO:root:total number of data: 709\n",
      "INFO:root:Reading vocab from ../data/bert_model/vocab.txt\n",
      "INFO:root:The number of vocab is 119547\n",
      "INFO:root:Build done\n",
      "INFO:root:{'batch_size': 256,\n",
      " 'epochs': 10000,\n",
      " 'fmax': 8000,\n",
      " 'hop_length': 250,\n",
      " 'kernel_size': 4,\n",
      " 'learning_rate': 0.001,\n",
      " 'max_len': 32,\n",
      " 'max_sec': 2,\n",
      " 'n_fft': 510,\n",
      " 'n_max': 100000,\n",
      " 'n_mels': 128,\n",
      " 'output_size': 256,\n",
      " 'sr_hub': 16000,\n",
      " 'sr_lab': 22050,\n",
      " 'top_db': 80.0,\n",
      " 'win_length': 510,\n",
      " 'window': 'hann'}\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(config=config, data='../data', n_max=None)\n",
    "config_file = os.path.join(dl.data, 'bert_model', 'bert_config.json')\n",
    "checkpoint_file = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dl.train_generator(data='hub', return_text=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 8924,\n",
       " 31401,\n",
       " 25503,\n",
       " 118671,\n",
       " 9682,\n",
       " 9539,\n",
       " 18784,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_embedding_table():\n",
    "    ckpt_loader = tf.train.load_checkpoint(checkpoint_file)\n",
    "#     model = keras_bert.load_trained_model_from_checkpoint(config_file=config_file,\n",
    "#                                                          checkpoint_file=checkpoint_file,\n",
    "#                                                          training=False,\n",
    "#                                                          trainable=None,\n",
    "#                                                          output_layer_num=1,\n",
    "#                                                          seq_len=dl.max_len)\n",
    "#     embed_table = keras_bert.get_token_embedding(model)\n",
    "#     del(model)\n",
    "    embed_table = ckpt_loader.get_tensor('bert/embeddings/word_embeddings')\n",
    "    del(ckpt_loader)\n",
    "    return embed_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import *\n",
    "\n",
    "keras = tf.keras\n",
    "\n",
    "BatchNorm = keras.layers.BatchNormalization\n",
    "Conv2D = keras.layers.Conv2D\n",
    "DConv = keras.layers.Conv2DTranspose\n",
    "Dense = keras.layers.Dense\n",
    "\n",
    "\n",
    "class AutoEncoder(keras.models.Model):\n",
    "    def __init__(self, config, input_shape, **kwargs):\n",
    "        super(AutoEncoder, self).__init__(**kwargs)\n",
    "        self.config = load_config(config)\n",
    "        self.audio_shape = input_shape\n",
    "\n",
    "        self.act_fn = keras.layers.LeakyReLU()\n",
    "        self.kernel_size = self.config['kernel_size']\n",
    "        \n",
    "        self.embedding_table = _get_embedding_table()\n",
    "\n",
    "        self.conv = [\n",
    "            Conv2D(filters=32, kernel_size=self.kernel_size, strides=(2, 2), padding='same', activation=None,\n",
    "                   input_shape=input_shape),\n",
    "            BatchNorm(),\n",
    "            self.act_fn,\n",
    "            Conv2D(filters=64, kernel_size=self.kernel_size, strides=(2, 2), padding='same', activation=None),\n",
    "            BatchNorm(),\n",
    "            self.act_fn,\n",
    "            Conv2D(filters=128, kernel_size=self.kernel_size, strides=(2, 2), padding='same', activation=None),\n",
    "            BatchNorm(),\n",
    "            self.act_fn,\n",
    "            Conv2D(filters=256, kernel_size=self.kernel_size, strides=(2, 2), padding='same', activation=None),\n",
    "            BatchNorm(),\n",
    "            self.act_fn,\n",
    "            Conv2D(filters=512, kernel_size=self.kernel_size, strides=(2, 2), padding='same', activation=None),\n",
    "            BatchNorm(),\n",
    "            self.act_fn,\n",
    "            #             keras.layers.Flatten(),\n",
    "            #             Dense(units=output_size, activation=None),\n",
    "            #             self.act_fn\n",
    "        ]\n",
    "\n",
    "        self.dconv = [\n",
    "            # 16\n",
    "            DConv(filters=512, kernel_size=self.kernel_size, strides=(2, 2), padding='same', activation=None),\n",
    "            BatchNorm(),\n",
    "            self.act_fn,\n",
    "            # 32\n",
    "            DConv(filters=256, kernel_size=self.kernel_size, strides=(2, 2), padding='same', activation=None),\n",
    "            BatchNorm(),\n",
    "            self.act_fn,\n",
    "            # 64\n",
    "            DConv(filters=128, kernel_size=self.kernel_size, strides=(2, 2), padding='same', activation=None),\n",
    "            BatchNorm(),\n",
    "            self.act_fn,\n",
    "\n",
    "            DConv(filters=64, kernel_size=self.kernel_size, strides=(2, 2), padding='same', activation=None),\n",
    "            BatchNorm(),\n",
    "            self.act_fn,\n",
    "\n",
    "            DConv(filters=1, kernel_size=self.kernel_size, strides=(2, 2), padding='same', activation=None),\n",
    "            keras.layers.ReLU(max_value=80.0)\n",
    "        ]\n",
    "        self.embed_hidden = Dense(units=self.hidden_size[0] * self.hidden_size[1])\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, **kwargs):\n",
    "        audio, text = inputs\n",
    "        audio = tf.expand_dims(audio, -1)\n",
    "\n",
    "        for layer in self.conv:\n",
    "            audio = layer(audio)\n",
    "            \n",
    "        audio = self._concat_text(audio, text)\n",
    "        for layer in self.dconv:\n",
    "            audio = layer(audio)\n",
    "        audio = tf.squeeze(audio, axis=-1)\n",
    "        \n",
    "        return audio\n",
    "    \n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        return int(self.audio_shape[0] / 2**5), int(self.audio_shape[1] / 2**5)\n",
    "    \n",
    "    def _concat_text(self, audio, text):\n",
    "        text = tf.gather(self.embedding_table, text)\n",
    "        text = self.embed_hidden(text)\n",
    "        text = tf.reshape(text, shape=[-1, 32, *self.hidden_size])\n",
    "        text = tf.transpose(text, perm=[0, 2, 3, 1])\n",
    "        audio = tf.concat([audio, text], axis=-1)\n",
    "        return audio\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(autoencoder)\n",
    "autoencoder = AutoEncoder(config=config, input_shape=dl.stft_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_audio = tf.data.Dataset.from_generator(partial(dl.train_generator, data='hub'), output_types=tf.float32)\n",
    "dataset_text = tf.data.Dataset.from_generator(partial(dl.train_generator, data='hub', return_text=True), output_types=tf.int32)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((dataset_audio, dataset_text))\n",
    "dataset = dataset.shuffle(buffer_size=dl.batch_size * 10).batch(16)\n",
    "\n",
    "inputs = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=15941, shape=(16, 256, 128), dtype=float32, numpy=\n",
       "array([[[1.70086582e-06, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.11190911e-05, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.07899396e-05, 0.00000000e+00, 1.63386198e-04, ...,\n",
       "         0.00000000e+00, 2.88854862e-05, 0.00000000e+00],\n",
       "        [6.64469917e-05, 9.95200753e-05, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         7.20243906e-06, 3.92513248e-05, 0.00000000e+00],\n",
       "        [1.98937778e-05, 3.19812261e-06, 0.00000000e+00, ...,\n",
       "         2.69277643e-06, 0.00000000e+00, 1.67227026e-05],\n",
       "        [0.00000000e+00, 9.46991076e-06, 8.35503761e-06, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[2.65257731e-05, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         3.97414115e-05, 0.00000000e+00, 0.00000000e+00],\n",
       "        [2.48286597e-06, 0.00000000e+00, 3.98206234e-04, ...,\n",
       "         0.00000000e+00, 1.97079644e-05, 0.00000000e+00],\n",
       "        [7.39248135e-05, 3.17955331e-04, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [7.09899314e-06, 3.88966037e-05, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 2.44637722e-05, 7.69293820e-06],\n",
       "        [8.09988614e-06, 2.79105334e-05, 2.19829803e-06, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.28239944e-05],\n",
       "        [0.00000000e+00, 1.14896129e-05, 1.86413672e-06, ...,\n",
       "         0.00000000e+00, 4.31489298e-07, 3.04824607e-06]],\n",
       "\n",
       "       [[1.90212177e-06, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 3.60659615e-04, ...,\n",
       "         7.66418452e-07, 1.24433398e-04, 0.00000000e+00],\n",
       "        [1.15983130e-04, 4.49720188e-04, 0.00000000e+00, ...,\n",
       "         2.19496651e-05, 0.00000000e+00, 4.57467149e-06],\n",
       "        ...,\n",
       "        [0.00000000e+00, 1.69198102e-05, 0.00000000e+00, ...,\n",
       "         4.01090119e-07, 3.61806124e-05, 0.00000000e+00],\n",
       "        [1.37266397e-05, 2.72431612e-06, 0.00000000e+00, ...,\n",
       "         1.37392226e-05, 0.00000000e+00, 2.04713092e-06],\n",
       "        [0.00000000e+00, 0.00000000e+00, 4.37487188e-06, ...,\n",
       "         5.22327764e-06, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[5.33094453e-06, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         6.00252461e-05, 0.00000000e+00, 0.00000000e+00],\n",
       "        [5.84477639e-05, 0.00000000e+00, 1.77100490e-04, ...,\n",
       "         1.13648159e-04, 9.58947276e-05, 2.13142976e-05],\n",
       "        [0.00000000e+00, 1.52107023e-04, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [8.37288724e-07, 9.96089057e-06, 0.00000000e+00, ...,\n",
       "         3.06660013e-06, 4.67293976e-05, 5.51561425e-06],\n",
       "        [1.48880299e-05, 2.50186658e-05, 0.00000000e+00, ...,\n",
       "         4.69779843e-06, 4.01816578e-05, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 4.07141397e-06, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[3.53515934e-05, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [9.87910025e-05, 0.00000000e+00, 1.27589243e-04, ...,\n",
       "         9.03835025e-05, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.07228545e-04, 1.30839311e-04, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 3.47524474e-05, 7.22040249e-06],\n",
       "        ...,\n",
       "        [1.49912330e-05, 5.89379015e-05, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.50614906e-05],\n",
       "        [1.51064542e-05, 7.59923932e-06, 1.38939313e-05, ...,\n",
       "         0.00000000e+00, 1.66664340e-05, 8.18747139e-06],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         3.00093143e-06, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[1.31883996e-06, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         5.18404777e-05, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 1.59747869e-04, ...,\n",
       "         6.86581188e-07, 1.56910988e-04, 0.00000000e+00],\n",
       "        [3.75393865e-05, 3.00957181e-04, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [3.67629309e-06, 1.84366581e-05, 0.00000000e+00, ...,\n",
       "         7.43838291e-06, 3.00660540e-05, 0.00000000e+00],\n",
       "        [3.93507389e-06, 3.20621293e-05, 0.00000000e+00, ...,\n",
       "         3.77613696e-06, 0.00000000e+00, 1.73084372e-05],\n",
       "        [0.00000000e+00, 9.53197286e-06, 1.37149154e-05, ...,\n",
       "         0.00000000e+00, 7.77047171e-06, 0.00000000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ckpt_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-61daf5f348bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mckpt_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_to_shape_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'embeddings'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# bert/embeddings/word_embeddings 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ckpt_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for key, value in ckpt_loader.get_variable_to_shape_map().items():\n",
    "    if 'embeddings' in key:\n",
    "        print(key, ':', value)\n",
    "    # bert/embeddings/word_embeddings 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
